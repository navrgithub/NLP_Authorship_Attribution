{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navrgithub/NLP_Authorship_Attribution/blob/main/POS%2B_RNN_%2BLSTM__task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJIch_VfwSp4",
        "outputId": "bf4eb482-354f-4d96-866e-978448e3d042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yQ1MJ2HlwSnH",
        "outputId": "686dad81-458f-49a0-822e-ebd26b8d35bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/NLP group project/data')"
      ],
      "metadata": {
        "id": "8GWPEuX0wSkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4Pj56oNRv0",
        "outputId": "aff10a3e-b1bc-4ba4-a2ad-40cdefc14ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "869PvF4WNeuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('final_task1_data.csv')"
      ],
      "metadata": {
        "id": "6n41G-m4Nexk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df[['text','label']]\n"
      ],
      "metadata": {
        "id": "6Be0yF6sNe6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download the required NLTK corpora\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "leuqECx7PlLL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data frame\n",
        "df = pd.read_csv('final_task1_data.csv')"
      ],
      "metadata": {
        "id": "2XKvHwvTVEBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for POS tagging\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Perform POS tagging on the words\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Return the POS tagged text\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "4DSOGsZ8VEMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the POS tagging function to the 'text' column\n",
        "df['pos_tags'] = df['Generation'].apply(pos_tagging)\n"
      ],
      "metadata": {
        "id": "bBYcyb0OVEpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels using the LabelEncoder class\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])"
      ],
      "metadata": {
        "id": "TzNB78oKVFL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['text','label']]"
      ],
      "metadata": {
        "id": "R9r36PdBPlaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, SimpleRNN, Concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
        "\n",
        "# Download the required NLTK corpora\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3PSIFtZSPliq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length of the input sequences\n",
        "max_length = 100\n",
        "\n",
        "# Define the number of training epochs\n",
        "epochs = 10\n",
        "\n",
        "# Define the number of units in the LSTM layer\n",
        "lstm_units = 128\n",
        "\n",
        "# Define the number of units in the RNN layer\n",
        "rnn_units = 64\n",
        "\n",
        "# Define the size of the embedding layer\n",
        "embedding_size = 128\n",
        "\n",
        "# Define the batch size for training\n",
        "batch_size = 32\n"
      ],
      "metadata": {
        "id": "-08Dz-lBVq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for POS tagging\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Perform POS tagging on the words\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Return the list of POS tags\n",
        "    return [tag for word, tag in pos_tags]\n",
        "\n",
        "# Apply the POS tagging function to the 'text' column\n",
        "df['pos_tags'] = df['text'].apply(pos_tagging)"
      ],
      "metadata": {
        "id": "qMfYu_THVq5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the POS tags and create the input and output sequences\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "pos_tags = set()\n",
        "\n",
        "for tags in df['pos_tags']:\n",
        "    for tag in tags:\n",
        "        pos_tags.add(tag)"
      ],
      "metadata": {
        "id": "Om7iDGZCVrIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_encoder = LabelEncoder()\n",
        "tag_encoder.fit(list(pos_tags))"
      ],
      "metadata": {
        "id": "JeqxbxTGVrKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tags in df['pos_tags']:\n",
        "    input_sequence = []\n",
        "    output_sequence = []\n",
        "    for i in range(len(tags)):\n",
        "        input_sequence.append(tags[i-1] if i > 0 else '')\n",
        "        output_sequence.append(tags[i])\n",
        "    input_sequences.append(input_sequence)\n",
        "    output_sequences.append(output_sequence)\n"
      ],
      "metadata": {
        "id": "VErA_EWaVrN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input and output sequences to integer sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_sequences)\n",
        "tokenized_input_sequences = tokenizer.texts_to_sequences(input_sequences)\n",
        "tokenized_output_sequences = tokenizer.texts_to_sequences(output_sequences)"
      ],
      "metadata": {
        "id": "nSWD8QTUVrQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the input and output sequences to the same length\n",
        "padded_input_sequences = pad_sequences(tokenized_input_sequences, maxlen=max_length, padding='post')\n",
        "padded_output_sequences = pad_sequences(tokenized_output_sequences, maxlen=max_length, padding='post')\n"
      ],
      "metadata": {
        "id": "Jkqp7aelVrTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the output sequences as one-hot vectors\n",
        "one_hot_output_sequences = np.array([pd.get_dummies(tag_encoder.transform(seq)) for seq in padded_output_sequences])\n"
      ],
      "metadata": {
        "id": "oDG-d28hVrVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the data into training, validation, and test sets\n",
        "train_input, test_input, train_output, test_output = train_test_split(padded_input_sequences, one_hot_output_sequences, test_size=0.2, random_state=42)\n",
        "train_input, val_input, train_output, val_output = train_test_split(train_input, train_output, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "R4Il23viWzv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, embedding_size, input_length=max_length))\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "model.add(SimpleRNN(rnn_units, return_sequences=True))\n",
        "model.add(Dense(len(pos_tags), activation='softmax'))"
      ],
      "metadata": {
        "id": "Ui1zf0TrW0OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model"
      ],
      "metadata": {
        "id": "CBNvvlpXVrXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_input, train_output, batch_size=batch_size, epochs=epochs, validation_data=(val_input, val_output))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_input, test_output, batch_size=batch_size)\n",
        "\n",
        "# Predict the POS tags for the test set\n",
        "y_pred = model.predict(test_input)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9F0FiQTU0Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the one-hot vectors to integer sequences\n",
        "y_true = [tag_encoder.inverse_transform(np.argmax(seq, axis=1)) for seq in test_output]\n",
        "y_pred = [tag_encoder.inverse_transform(np.argmax(seq, axis=1)) for seq in y_pred]\n",
        "\n",
        "# Flatten the integer sequences\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "LfOxyXJ7Xbme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F1 score, accuracy, recall, and precision\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test loss: {:.3f}'.format(test_loss))\n",
        "print('Test accuracy: {:.3f}'.format(test_accuracy))\n",
        "print('F1 score: {:.3f}'.format(f1))\n",
        "print('Accuracy: {:.3f}'.format(accuracy))\n",
        "print('Recall: {:.3f}'.format(recall))\n",
        "print('Precision: {:.3f}'.format(precision))"
      ],
      "metadata": {
        "id": "vHWRNBXKXd9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}