{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navrgithub/NLP_Authorship_Attribution/blob/main/Baseline_Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62tRDYmyIlSu",
        "outputId": "43d8e14b-2a51-4ae3-8d60-da943584057f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Generation        label\n",
            "0     Latest Headlines on CNN Business the great shu...        human\n",
            "1     China wants to take a victory lap over its han...        human\n",
            "2     Coronavirus disinformation creates challenges ...        human\n",
            "3     China coronavirus: Eating wild animals made il...        human\n",
            "4     China's economy could shrink for the first tim...        human\n",
            "...                                                 ...          ...\n",
            "1061         100%. All parts of your body are your own.  InstructGPT\n",
            "1062  1. Establish regular cleaning schedules for al...  InstructGPT\n",
            "1063  In some cities around the world, including Tok...  InstructGPT\n",
            "1064  . The BBC News app is a comprehensive and easy...  InstructGPT\n",
            "1065  The BBC is implementing several steps to stren...  InstructGPT\n",
            "\n",
            "[11727 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd \n",
        "    \n",
        "# making dataframe \n",
        "df_human = pd.read_csv(\"new_human.csv\") \n",
        "df_human.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_human)\n",
        "\n",
        "df_ctrl = pd.read_csv(\"new_ctrl.csv\") \n",
        "df_ctrl.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_ctrl)\n",
        "\n",
        "df_gpt = pd.read_csv(\"new_gpt.csv\") \n",
        "df_gpt.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_gpt)\n",
        "\n",
        "df_gpt2 = pd.read_csv(\"new_gpt2.csv\") \n",
        "df_gpt2.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_gpt2)\n",
        "\n",
        "df_grover = pd.read_csv(\"new_grover.csv\") \n",
        "df_grover.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_grover)\n",
        "\n",
        "df_xlm = pd.read_csv(\"new_xlm.csv\") \n",
        "df_xlm.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_xlm)\n",
        "\n",
        "df_xlnet = pd.read_csv(\"new_xlnet.csv\") \n",
        "df_xlnet.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_xlnet)\n",
        "\n",
        "df_pplm = pd.read_csv(\"new_pplm.csv\") \n",
        "df_pplm.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_pplm)\n",
        "\n",
        "df_fair = pd.read_csv(\"new_fair.csv\") \n",
        "df_fair.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "# print(df_fair)\n",
        "\n",
        "df_gpt3 = pd.read_csv(\"final_text_curie_output.csv\") \n",
        "df_gpt3.drop(columns=[\"Prompts\"], inplace=True)\n",
        "df_gpt3.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "df_gpt3 = df_gpt3.rename(columns={'Responses': 'Generation', 'Model Name': 'label'})\n",
        "# print(df_gpt3)\n",
        "\n",
        "df_instructgpt = pd.read_csv(\"final_instructgpt_results.csv\") \n",
        "df_instructgpt.drop(columns=[\"Title\"], inplace=True)\n",
        "df_instructgpt.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "df_instructgpt = df_instructgpt.rename(columns={'InstructGPT_result': 'Generation', 'Model': 'label'})\n",
        "# print(df_instructgpt)\n",
        "\n",
        "result = pd.concat([df_human, df_ctrl, df_gpt, df_gpt2, df_grover,df_xlm, df_xlnet, df_pplm, df_fair, df_gpt3, df_instructgpt], axis=0)\n",
        "result.drop(columns=[\"number\"], inplace=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpic5cnJIq5T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "result = result. replace(np. nan,'',regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIqNktYEJ_x9",
        "outputId": "0dcaba93-4603-401e-bff6-ee19e11721b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "num_nan1 = result['Generation'].isna().sum()\n",
        "print(num_nan1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU1LAfUXJ_3b",
        "outputId": "a882859f-090c-491f-b039-194f97bee056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# First, we need to preprocess the text data by tokenizing it and converting it to a bag-of-words representation\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the text data\n",
        "result['tokenized_data'] = result['Generation'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Convert the tokenized data to a bag-of-words representation using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(result['Generation'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, result['label'], test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9UlqoK8J_6q",
        "outputId": "c12edf88-46a7-4ae4-8066-8b554ab166af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihQbRnX7J__V",
        "outputId": "d5593d8b-79a3-47e4-ff94-42a3fa8bfcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xlnet' 'InstructGPT' 'xlnet' ... 'fair' 'gpt' 'gpt']\n",
            "Accuracy: 0.76\n",
            "Precision: 0.77\n",
            "Recall: 0.76\n",
            "F1-score: 0.77\n"
          ]
        }
      ],
      "source": [
        "# Create an SVM model and fit the model to the training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "svm_model = SVC(kernel='linear', C=1, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "# print(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "# Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvqbK5iYKACO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda789fc-4190-4e79-8abb-70f9d7a160df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n",
            "Precision: 0.76\n",
            "Recall: 0.75\n",
            "F1-score: 0.75\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a Random Forest model and fit the model to the training data\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QhWRCG4KAFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6039a155-6214-4c65-bb2e-662c72f69727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "def preprocess(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text into individual words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "\n",
        "    # Join the words back into a string\n",
        "    preprocessed_text = \" \".join(words)\n",
        "\n",
        "    return preprocessed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRFyM_0UKAHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7620e2c-094c-40f3-97bb-1d279075996f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the input text\n",
        "input_text = \"This is a test sentence.\"\n",
        "preprocessed_text = preprocess(input_text)\n",
        "\n",
        "# Convert the preprocessed text into a feature vector\n",
        "vectorized_text = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Use the trained Random Forest model to predict the label\n",
        "predicted_label = rf_model.predict(vectorized_text)[0]\n",
        "\n",
        "print(predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmscG-UGKAI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c5782d-9289-4507-b08e-3e276a8bb07f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Latest Headlines on CNN Business the great shu...\n",
              "1       China wants to take a victory lap over its han...\n",
              "2       Coronavirus disinformation creates challenges ...\n",
              "3       China coronavirus: Eating wild animals made il...\n",
              "4       China's economy could shrink for the first tim...\n",
              "                              ...                        \n",
              "1061           100%. All parts of your body are your own.\n",
              "1062    1. Establish regular cleaning schedules for al...\n",
              "1063    In some cities around the world, including Tok...\n",
              "1064    . The BBC News app is a comprehensive and easy...\n",
              "1065    The BBC is implementing several steps to stren...\n",
              "Name: Generation, Length: 11727, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "result['Generation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pjZZCdmKANr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b560b2b-def6-4a20-fcab-25f1dbe7c82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n",
            "Precision: 0.75\n",
            "Recall: 0.75\n",
            "F1-score: 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a logistic regression model and fit the model to the training data\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create a Multinomial Naive Bayes model and fit the model to the training data\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLMuo6F29wPw",
        "outputId": "0581076c-0daa-4c04-a885-376b11a0cc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.52\n",
            "Precision: 0.61\n",
            "Recall: 0.52\n",
            "F1-score: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EU3LvX7j_J23"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXeRfb7UDAZKScqrqSaQLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}